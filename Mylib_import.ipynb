{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "import dlib\n",
    "from imutils import face_utils\n",
    "import open3d as o3d\n",
    "import pyrealsense2 as rs\n",
    "import scipy.ndimage\n",
    "import copy\n",
    "#import pptk\n",
    "from skimage.color import rgb2hsv\n",
    "from scipy import signal\n",
    "import colorsys\n",
    "from squaternion import Quaternion\n",
    "import random\n",
    "import time\n",
    "import xlsxwriter \n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import glob\n",
    "import matplotlib.image as mpimg\n",
    "from scipy.io import savemat\n",
    "import imutils\n",
    "import pandas as pd  # Import pandas library\n",
    "import ast\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder(folder_name):\n",
    "    try:\n",
    "        os.mkdir(folder_name)\n",
    "        print(f\"Folder '{folder_name}' created successfully.\")\n",
    "    except FileExistsError:\n",
    "        print(f\"Folder '{folder_name}' already exists.\")\n",
    "        \n",
    "\n",
    "def sort_files_numerically(file):\n",
    "    # Extract all the numbers from the filename\n",
    "    numbers = [int(num) for num in re.findall(r'\\d+', os.path.basename(file))]\n",
    "    return numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_scaling_matrix(scale_factor):\n",
    "    return np.array([\n",
    "        [scale_factor, 0, 0, 0],\n",
    "        [0, scale_factor, 0, 0],\n",
    "        [0, 0, scale_factor, 0],\n",
    "        [0, 0, 0, 1]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_camera_intrinsics(serial_number):\n",
    "    pipeline = rs.pipeline()\n",
    "    config = rs.config()\n",
    "    config.enable_device(serial_number)\n",
    "    config.enable_stream(rs.stream.color, 640, 480, rs.format.rgb8, 30)\n",
    "    config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "    profile = pipeline.start(config)\n",
    "\n",
    "    # Get color and depth stream profiles\n",
    "    color_stream_profile = profile.get_stream(rs.stream.color)\n",
    "    depth_stream_profile = profile.get_stream(rs.stream.depth)\n",
    "\n",
    "    # Get color and depth intrinsics\n",
    "    intrinsics_color = color_stream_profile.as_video_stream_profile().get_intrinsics()\n",
    "    intrinsics_depth = depth_stream_profile.as_video_stream_profile().get_intrinsics()\n",
    "\n",
    "    # Stop the pipeline\n",
    "    pipeline.stop()\n",
    "    \n",
    "    print(\"\\nIntrinsics for Color Stream:\")\n",
    "    print(\"Width:\", intrinsics_color.width)\n",
    "    print(\"Height:\", intrinsics_color.height)\n",
    "    print(\"Fx:\", intrinsics_color.fx)\n",
    "    print(\"Fy:\", intrinsics_color.fy)\n",
    "    print(\"Cx:\", intrinsics_color.ppx)\n",
    "    print(\"Cy:\", intrinsics_color.ppy)\n",
    "\n",
    "    print(\"\\nIntrinsics for Depth Stream:\")\n",
    "    print(\"Width:\", intrinsics_depth.width)\n",
    "    print(\"Height:\", intrinsics_depth.height)\n",
    "    print(\"Fx:\", intrinsics_depth.fx)\n",
    "    print(\"Fy:\", intrinsics_depth.fy)\n",
    "    print(\"Cx:\", intrinsics_depth.ppx)\n",
    "    print(\"Cy:\", intrinsics_depth.ppy)\n",
    "\n",
    "    return intrinsics_color, intrinsics_depth\n",
    "\n",
    "def resize_intrinsics(intrinsics, original_width, original_height, new_width, new_height):\n",
    "    intrinsics_resized = rs.intrinsics()\n",
    "    \n",
    "    # Calculate scaling factors\n",
    "    scale_x = new_width / original_width\n",
    "    scale_y = new_height / original_height\n",
    "    \n",
    "    # Adjust intrinsic parameters\n",
    "    intrinsics_resized.width = new_width\n",
    "    intrinsics_resized.height = new_height\n",
    "    intrinsics_resized.fx = intrinsics.fx * scale_x\n",
    "    intrinsics_resized.fy = intrinsics.fy * scale_y\n",
    "    intrinsics_resized.ppx = intrinsics.ppx * scale_x\n",
    "    intrinsics_resized.ppy = intrinsics.ppy * scale_y\n",
    "    \n",
    "    return intrinsics_resized\n",
    "\n",
    "\n",
    "def get_depth_to_color_extrinsics(serial_number): #for D435i (L515 no need this transformation)\n",
    "    # Create a pipeline\n",
    "    pipeline = rs.pipeline()\n",
    "    config = rs.config()\n",
    "    config.enable_device(serial_number)\n",
    "    config.enable_stream(rs.stream.color, 640, 480, rs.format.rgb8, 30)\n",
    "    config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "    profile = pipeline.start(config)\n",
    "\n",
    "    # Get depth sensor extrinsics to color stream\n",
    "    extrinsics = profile.get_stream(rs.stream.depth).get_extrinsics_to(profile.get_stream(rs.stream.color))\n",
    "\n",
    "    # Stop the pipeline\n",
    "    pipeline.stop()\n",
    "\n",
    "    return extrinsics\n",
    "\n",
    "def rt_to_T(rotation,translation):\n",
    "    # Create a 3x3 rotation matrix\n",
    "    rotation_matrix = np.array(rotation).reshape(3, 3)\n",
    "\n",
    "    # Create a 4x4 transformation matrix\n",
    "    transformation_matrix = np.eye(4)\n",
    "    transformation_matrix[:3, :3] = rotation_matrix\n",
    "    transformation_matrix[:3, 3] = translation\n",
    "    return transformation_matrix\n",
    "\n",
    "def flip_uptodown_matrix(rotation_angle):\n",
    "    flip_updown = np.array([\n",
    "        [np.cos(np.radians(rotation_angle)), 0, -np.sin(np.radians(rotation_angle)), 0],\n",
    "        [0, 1, 0, 0],\n",
    "        [np.sin(np.radians(rotation_angle)), 0, np.cos(np.radians(rotation_angle)), 0],\n",
    "        [0, 0, 0, -1]\n",
    "    ])\n",
    "    return flip_updown\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #aligb rgb and depth to visualize in images (quite difficilt way selecting depth_scale)\n",
    "def align_depth_to_color(depth_image, color_image, depth_intrinsics, color_intrinsics, depth_to_color_extrinsics, depthscale):\n",
    "    aligned_depth = np.zeros_like(np.asarray(color_image))\n",
    "    \n",
    "    for y in range(depth_intrinsics.height):  \n",
    "        for x in range(depth_intrinsics.width):\n",
    "            depth_value = np.asarray(depth_image)[y, x]* depthscale\n",
    "            if depth_value > 0:  # Skip invalid depth values\n",
    "                depth_point = np.array([[(x - depth_intrinsics.ppx) * depth_value / depth_intrinsics.fx],\n",
    "                                        [(y - depth_intrinsics.ppy) * depth_value / depth_intrinsics.fy],\n",
    "                                        [depth_value],\n",
    "                                        [1.0]])\n",
    "                transformed_point = np.dot(depth_to_color_extrinsics, depth_point)\n",
    "                projected_point = transformed_point[:3] / transformed_point[2]\n",
    "                \n",
    "                color_x = int(projected_point[0] * color_intrinsics.fx / projected_point[2] + color_intrinsics.ppx + 0.5)\n",
    "                color_y = int(projected_point[1] * color_intrinsics.fy / projected_point[2] + color_intrinsics.ppy + 0.5)\n",
    "                \n",
    "                if 0 <= color_x < color_intrinsics.width and 0 <= color_y < color_intrinsics.height:\n",
    "                    aligned_depth[y, x] = np.asarray(color_image)[color_y, color_x]\n",
    "                    \n",
    "\n",
    "    \n",
    "    return aligned_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for \"Arisa_PCRegist_SIFT\n",
    "\n",
    "**1.Generating a random initial alignment between two point clouds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random initial alignment\n",
    "def random_initial_alignment(source: o3d.geometry.PointCloud, translation_bound: float, rotation_bound: float) -> o3d.geometry.PointCloud:\n",
    "    # Create a copy of the source to avoid modifying the original point cloud\n",
    "    transformed_source = copy.deepcopy(source)\n",
    "\n",
    "    # Generate random translation within bounds\n",
    "    translation = np.random.uniform(-translation_bound, translation_bound, 3)\n",
    "\n",
    "    # Generate random rotation within bounds\n",
    "    rx = np.random.uniform(-rotation_bound, rotation_bound)\n",
    "    ry = np.random.uniform(-rotation_bound, rotation_bound)\n",
    "    rz = np.random.uniform(-rotation_bound, rotation_bound)\n",
    "    rotation = o3d.geometry.get_rotation_matrix_from_xyz((rx, ry, rz))\n",
    "\n",
    "    # Compute the transformation matrix\n",
    "    transformation = np.eye(4)\n",
    "    transformation[:3, :3] = rotation\n",
    "    transformation[:3, 3] = translation\n",
    "\n",
    "    # Apply transformation to the source point cloud\n",
    "    transformed_source.transform(transformation)\n",
    "\n",
    "    return transformed_source, transformation\n",
    "\n",
    "def random_initial(source_point_cloud, target_point_cloud):\n",
    "    # Calculate bounding boxes of both point clouds\n",
    "    source_bbox = source_point_cloud.get_axis_aligned_bounding_box()\n",
    "    target_bbox = target_point_cloud.get_axis_aligned_bounding_box()\n",
    "\n",
    "    # Manually compute the diagonal length of bounding boxes\n",
    "    source_diagonal_length = np.linalg.norm(np.asarray(source_bbox.max_bound) - np.asarray(source_bbox.min_bound))\n",
    "    target_diagonal_length = np.linalg.norm(np.asarray(target_bbox.max_bound) - np.asarray(target_bbox.min_bound))\n",
    "\n",
    "    # Determine translation bounds\n",
    "    # Set the translation bounds as a fraction of the diagonal length of bounding boxes\n",
    "    translation_bound = max(source_diagonal_length, target_diagonal_length) * 0.1  # 10% of the maximum diagonal length\n",
    "\n",
    "    # Determine rotation bounds\n",
    "    # Here, we use a small fixed rotation bound (10 degrees converted to radians)\n",
    "    rotation_bound = np.radians(10)\n",
    "\n",
    "    randinit_source, randinit_transformation = random_initial_alignment(source_point_cloud, translation_bound, rotation_bound)\n",
    "#     print(f\"translation_bound:{translation_bound}, rotation_bound:{rotation_bound}\")\n",
    "#     print(\"random initial transformation:\\n\",randinit_transformation)\n",
    "    return randinit_source, randinit_transformation\n",
    "\n",
    "# randinit_source, randinit_transformation = random_initial(source_point_cloud, target_point_cloud)\n",
    "# o3d.visualization.draw_geometries([source_point_cloud, target_point_cloud], width=800, height=600)\n",
    "# o3d.visualization.draw_geometries([randinit_source, target_point_cloud], width=800, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.Point cloud registration with SIFT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Keypoints extraction\n",
    "#1.1 When point cloud has no colors then use FPFH features\n",
    "def check_pc_has_color(pcd):\n",
    "    # Check if the point cloud has colors\n",
    "    if len(pcd.colors) > 0:\n",
    "        # print(\"The point cloud has color information.\")\n",
    "        return 'Yes'\n",
    "    else:\n",
    "       #  print(\"The point cloud does not have color information.\")\n",
    "        return 'No'\n",
    "\n",
    "def compute_fpfh_features(pcd, search_radius=0.25):\n",
    "    #Extract non-colors point clouds\n",
    "    pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=search_radius, max_nn=100))\n",
    "    fpfh = o3d.registration.compute_fpfh_feature(pcd, o3d.geometry.KDTreeSearchParamHybrid(radius=search_radius, max_nn=100))\n",
    "    return fpfh\n",
    "\n",
    "#1.2 When point cloud has colors then use SIFT features\n",
    "def point_cloud_to_color_and_point_map(pcd, img_dim=200):  # 200 #How dim should be set?!\n",
    "    # Convert point cloud data to numpy array\n",
    "    points = np.asarray(pcd.points)\n",
    "    colors = np.asarray(pcd.colors) * 255  # Scaling to 0-255 range\n",
    "\n",
    "    # Normalize x, y to image dimensions\n",
    "    normalized_points = (((points - points.min(axis=0)) / (points.max(axis=0) - points.min(axis=0))) * (img_dim - 1)).astype(int)\n",
    "    img_coords = normalized_points[:, :2].astype(int)\n",
    "\n",
    "    # Create color map and count map for averaging\n",
    "    color_map = np.zeros((img_dim, img_dim, 3))\n",
    "    count_map = np.zeros((img_dim, img_dim, 3))\n",
    "\n",
    "    # Create point_map to store 3D coordinates\n",
    "    point_map = np.zeros((img_dim, img_dim, 3))\n",
    "\n",
    "    for img_coord, color, point in zip(img_coords, colors, points):\n",
    "        # Check if the indices are within the valid range\n",
    "        if 0 <= img_coord[1] < img_dim and 0 <= img_coord[0] < img_dim:\n",
    "            color_map[img_coord[1], img_coord[0]] += color  # accumulate color values\n",
    "            count_map[img_coord[1], img_coord[0]] += 1  # count the number of points per bin\n",
    "            # For point map, just store the 3D point for now without averaging\n",
    "            # This could be changed based on specific requirements\n",
    "            point_map[img_coord[1], img_coord[0]] = point\n",
    "\n",
    "    # Average the accumulated color values\n",
    "    avg_mask = count_map > 0\n",
    "    color_map[avg_mask] /= count_map[avg_mask]\n",
    "\n",
    "    return color_map.astype(np.uint8), point_map\n",
    "\n",
    "\n",
    "def extract_sift_feat(img):\n",
    "    gray = cv2.cvtColor(img.astype(np.uint8), cv2.COLOR_BGR2GRAY)\n",
    "    sift = cv2.xfeatures2d.SIFT_create(contrastThreshold=0.08, edgeThreshold=0.2)\n",
    "    keypoints, descriptors = sift.detectAndCompute(gray, None)\n",
    "    return keypoints, descriptors\n",
    "\n",
    "def draw_keypoints(img, keypoints, title='Keypoints'):\n",
    "    img_with_keypoints = cv2.drawKeypoints(img, keypoints, outImage=np.array([]), color=(0, 255, 0))\n",
    "    cv2.imshow(title, img_with_keypoints)\n",
    "\n",
    "def project_keypoints_to_3D(keypoints, descriptors, point_map):\n",
    "    keypoints_3d = []\n",
    "    descriptors_3d = []\n",
    "    for i, keypoint in enumerate(keypoints):\n",
    "        x, y = np.round(keypoint.pt).astype(int)\n",
    "        keypoints_3d.append(point_map[y, x])\n",
    "        descriptors_3d.append(descriptors[i])\n",
    "    return keypoints_3d, np.array(descriptors_3d)\n",
    "\n",
    "def visualize_keypoints(point_cloud, keypoints_3d):\n",
    "    # Convert keypoints to Open3D format\n",
    "    keypoints_o3d = o3d.geometry.PointCloud()\n",
    "    keypoints_o3d.points = o3d.utility.Vector3dVector(np.array(keypoints_3d))\n",
    "    # Set the color of the keypoints to red\n",
    "    keypoints_o3d.paint_uniform_color([1, 0, 0])\n",
    "    # Visualize the point cloud and keypoints\n",
    "#     o3d.visualization.draw_geometries([point_cloud, keypoints_o3d], width=800, height=600)\n",
    "\n",
    "def find_good_matches(src_descriptors, tar_descriptors):\n",
    "    try:\n",
    "        # Check if descriptors are empty\n",
    "        if src_descriptors is None or tar_descriptors is None:\n",
    "            print(\"Descriptors are empty.\")\n",
    "            return []\n",
    "        \n",
    "        # Ensure descriptors are of type float32\n",
    "        src_descriptors = src_descriptors.astype(np.float32)\n",
    "        tar_descriptors = tar_descriptors.astype(np.float32)\n",
    "        \n",
    "        # BFMatcher with default params\n",
    "        bf = cv2.BFMatcher()\n",
    "        matches = bf.knnMatch(src_descriptors, tar_descriptors, k=2)\n",
    "        \n",
    "        # Apply ratio test\n",
    "        good_matches = []\n",
    "        for match in matches:\n",
    "            if len(match) >= 2:  # Ensure there are at least two matches to unpack\n",
    "                m, n = match\n",
    "                if m.distance < 0.75 * n.distance:\n",
    "                    good_matches.append(m)\n",
    "            else:\n",
    "                print(\"Not enough matches found to apply ratio test.\")\n",
    "                \n",
    "        return good_matches\n",
    "        \n",
    "    except cv2.error as e:\n",
    "        print(f\"An OpenCV error occurred: {e}\")\n",
    "        return []\n",
    "    \n",
    "def extract_matched_keypoints_descriptors(matches, src_keypoints_3d, tar_keypoints_3d, src_descriptors, tar_descriptors):\n",
    "    matched_src_keypoints = [src_keypoints_3d[m.queryIdx] for m in matches]\n",
    "    matched_tar_keypoints = [tar_keypoints_3d[m.trainIdx] for m in matches]\n",
    "    \n",
    "    matched_src_descriptors = [src_descriptors[m.queryIdx] for m in matches]\n",
    "    matched_tar_descriptors = [tar_descriptors[m.trainIdx] for m in matches]\n",
    "    \n",
    "    # Converting lists of keypoints to numpy arrays\n",
    "    matched_src_keypoints = np.array(matched_src_keypoints)\n",
    "    matched_tar_keypoints = np.array(matched_tar_keypoints)\n",
    "    \n",
    "    return matched_src_keypoints, matched_tar_keypoints, matched_src_descriptors, matched_tar_descriptors\n",
    "\n",
    "#2. Matching Keypoints\n",
    "def apply_ransac(src_keypoints_3d, tar_keypoints_3d, src_descriptors, tar_descriptors):\n",
    "    try:\n",
    "        if len(src_keypoints_3d) == 0:\n",
    "            raise ValueError(\"src_keypoints_3d is empty.\")\n",
    "\n",
    "        \n",
    "        src_pcd = o3d.geometry.PointCloud()\n",
    "        src_pcd.points = o3d.utility.Vector3dVector(np.array(src_keypoints_3d))\n",
    "\n",
    "        tar_pcd = o3d.geometry.PointCloud()\n",
    "        tar_pcd.points = o3d.utility.Vector3dVector(np.array(tar_keypoints_3d))\n",
    "\n",
    "        # Ensuring the descriptors are numpy arrays\n",
    "        src_descriptors = np.array(src_descriptors)\n",
    "        tar_descriptors = np.array(tar_descriptors)\n",
    "\n",
    "        # Creating Open3D Feature objects from the descriptors\n",
    "        src_feature = o3d.registration.Feature()\n",
    "        src_feature.data = src_descriptors.T  # Transpose due to Open3D's [dim, num_points] format\n",
    "\n",
    "        tar_feature = o3d.registration.Feature()\n",
    "        tar_feature.data = tar_descriptors.T\n",
    "\n",
    "        # Applying RANSAC based on feature matching\n",
    "        result = o3d.registration.registration_ransac_based_on_feature_matching(\n",
    "            source=src_pcd,\n",
    "            target=tar_pcd,\n",
    "            source_feature=src_feature,\n",
    "            target_feature=tar_feature,\n",
    "            max_correspondence_distance=0.03,\n",
    "            estimation_method=o3d.registration.TransformationEstimationPointToPoint(False)\n",
    "        )\n",
    "        return result.transformation\n",
    "    except ValueError as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse(transformed_src, tar):\n",
    "    distances = transformed_src.compute_point_cloud_distance(tar)\n",
    "    rmse = np.sqrt(np.mean(np.square(distances)))\n",
    "    return rmse\n",
    "\n",
    "def compute_differences(transformed_src, tar):\n",
    "    # Find nearest neighbors\n",
    "    pcd_tree = o3d.geometry.KDTreeFlann(tar)\n",
    "    src_colors = np.asarray(transformed_src.colors)\n",
    "    \n",
    "    color_differences = []\n",
    "    position_differences = []\n",
    "    \n",
    "    for i, point in enumerate(transformed_src.points):\n",
    "        _, idx, _ = pcd_tree.search_knn_vector_3d(point, 1)\n",
    "        \n",
    "        # Calculate the color differences\n",
    "        src_color = np.asarray(transformed_src.colors)[i]\n",
    "        tar_color = np.asarray(tar.colors)[idx[0]]\n",
    "        color_difference = np.linalg.norm(src_color - tar_color)\n",
    "        color_differences.append(color_difference)\n",
    "        \n",
    "        # Calculate the position differences\n",
    "        tar_point = np.asarray(tar.points)[idx[0]]\n",
    "        position_difference = np.linalg.norm(np.array(point) - np.array(tar_point))\n",
    "        position_differences.append(position_difference)\n",
    "    \n",
    "    return np.mean(color_differences), np.mean(position_differences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
